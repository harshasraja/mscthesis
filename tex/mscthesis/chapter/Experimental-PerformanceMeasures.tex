%à¤¬
\section{Performance Indicators} \label{sexp:PerformanceIndicators}
% Performance of database systems is commonly measured in terms of the
% \textit{Response time} and \textit{Throughput}.  

Response time and throughput are the indicators used to gauge the
performance of the four solutions under the implemented \ac{API} and the
respective referential integrity constraints specified by the example
application.  Response time refers to the time a \ac{DBMS}  takes to process an
operation and produce results to the end user~\citep{boral} (\todo{cite Demurjian, 
Berkely, serverside,  }).  Throughput  refers to  the number of operations that can be
processed by the \ac{DBMS} in a unit of time. 

In the experiments,  the response time is computed by dividing the total
execution time of an operation for a set of entities by the number of entities.  In other
words,  the response time measures the amount of time required to perform a
single operation on one entity.  On the other hand,  the throughput is the
inverse of the response time and is computed accordingly by dividing the number of entities by
the total execution time of an operation for a set of entities.  In other words, 
the throughput measures the number of operations that are performed in a unit
of time.  The following equations define the response time $r$ and throughput
$t$,
 

% \begin{tabular}{cc}
% $\displaystyle r = \frac{1}{n}\sum_{i = 1}^{n}{o_i} \label{eq:response-time}$ &
% $\displaystyle t = n / \sum_{i = 1}^{n}{o_i} \label{eq:throughput}$
% \end{tabular}

\begin{equation*}
\begin{gathered}
\displaystyle r = \frac{1}{n}\sum_{i = 1}^{n}{o_i}
\end{gathered} \hspace{2cm}
\begin{gathered}
t = n / \sum_{i = 1}^{n}{o_i}  
\end{gathered}
\end{equation*}

% \begin{align}
% \displaystyle
% 	 
% \end{align}

\noindent where $o_i$ is the time for an operation over entity $i$,  and  $n$ is
the number of entities. 


Notice that external variables such as network latency,   simultaneous processes
in the operating systems of each node,   and other variables are not considered
for the analysis of results.   Even when they are present,   it is expected that
results will not be significantly biased by them.   Nonetheless,   the
experiments will be  performed at night time over weekends as this is the time
 when the cluster is least used,   thus reducing the presence of such variables
and hence their impact on the results.  



% In the experiments,  the throughput of all the operations triggering referential
%  integrity validation across all solutions is measured as operations per second. 
%  A single operation stands for each time an entity is inserted or updated or
%  deleted.  Note that only the operations that introduce the referential integrity
%  validation are measured and thus \texttt{read} operations are not measured in
%  terms of response time or throughput.    


  


% Measuring response time for a
% database operation is similar to a black-box evaluation because it is measured 
% without considering the internal functioning  of the database system.   According
% to (\todo{cite Demurjian}) such an evaluation is ideal for a complete database
% system to measure its performance and to give the users details about its 
% efficiency and speed in performing operations.   
% The response time of Cassandra when such validations
% are not in place is also measured and considered as a baseline with which to
% analyse the solutions.   Such a comparison determines the degree of change in
% speed of Cassandra when such overheads are introduced and gives users useful
% information about how each solution affects the performance of the database
% system.  



% For example,   inserting 1000 students means that 1000 \texttt{insert}
% operations are processed by Cassandra.  

% The traditional TPC benchmarks are not considered as performance measures in
% this experiment  because these benchmarks are centred around transactions and
% OLTP workloads.   The principal metrics for these benchmarks are the transaction
% rate,   query per hour,   cost indicators of a system,   among others,   which are
% suitable indicators for \ac{DBMS} with ACID properties~\citep{TPC}.   Hence,   for
% assessing Cassandra which lacks SQL queries and  ACID properties,   these
% benchmarks are not suitable indicators of performance. 


%  it is
% essential to measure it in terms of what is critical to application  using Cassandra.   In this experiment it is critical to
% measure the difference in time for an operation to complete in Cassandra when
% referential integrity validation is activated or not activated.  


% These operations which trigger referential integrity validation for an entity
% namely the \texttt{insert},   \texttt{update},   \texttt{delete} operations are
% were measured in terms of the throughput in the experiments.   Throughout
% commonly referes to the number of operations performed

% It has to be noted that the operations are prone to  external factors like
% network latency,   bandwidth,   network routing,   network workload among others
% which typically affect a network consisting of several machines and users.  
% This is because the Cassandra cluster used in the experiments is deployed over
% a network that is used by many users concurrently thus exposing the operations
% to such factors.   Identifying such factors and analysing them is beyond the
% scope of this thesis and the analysis is strictly in terms of how the metadata
% storage and referential integrity validation affects Cassandra's performance.  
% It is a general practise for applications to incorporate code within
% applications to log the timestamps for transactions in traditional
% \acp{DBMS}~\citep{IBMPerformance}.  


% In order to determine the response time and throughput,   the output log files are
% are analysed using R.   (\todo{explain how it is imported to R and graphs
% produced--SOS Juan!})



