\chapter{Experimental Design}
\todo{Check number of enrolments, introduce all metadata, and retrieve 
details of cassandra cluster from my computer}

\todo{summary of chapter structure}


\section{University example}
University example (Metadata) present example again and all metadata.

\section{Cassandra cluster}

The environment to deploy cassandra is  an homogeneous cluster conformed by 10
nodes. That is, all 10 nodes have the same characteristics in software and
hardware. These nodes emulate a cloud environment in which each node runs
Cassandra. The characteristics of these nodes are:

\begin{itemize}
  \item Hardware
  \item Software
\end{itemize}




\section{Experimental setup}\label{s:exp:setup}


 The experimentation consists
on performing 100 runs where artificial data is randomly created, updated and deleted from column famlies in a Cassandra cluster.
On each run, the time required for each operation is recorded in order to assess the
performance of each solution as well as the throughput in terms of operations
per unit of time. Notice that each operation on the artificial data  is
performed in a batch, and entities from each column family are randomly sorted
before any operation takes place. Random sorting is performed in order to
prevent the results to be biased from possible optimization made by Cassandra in
terms of indexes or other criteria.
		
The artificial data is made up of 1000 students, 1000 courses, and 10000
enrolments which are the result of assigning 10 different courses to each
student. Courses are assigned by dividing the number of courses
into 100 groups of 10 courses each, and assigning a group for each student.
Notice that such an assignment involves that X students have the same courses
assigned. The quantity of records to be inserted for each entity was chosen
considering an overall reasonable time for completing the experimentation of all
solutions.
		
The format of the artificial data created is as follows. Student has a
unit-increasing studentId, which is merged into the fields firstName and
lastName as "First Name (studentId)" and "Last Name (studentId)", and trimester
and level are random numbers. Course has a   unit-increasing courseId which is
appended to the prefix "COMP", it also has a composed course name as in
the student (merging id and field). Enrolment contains a unit-increasing rowId, and the
respective foreign keys of student and course.
		
The order of the operations performed on the data is as follows. \textbf{Create}
inserts all the students, courses and enrolments. \textbf{Update} performs
changes on the primary keys of students and courses, and on the foreign keys
of enrolment (the one relative to courses, especifically). Finally,
\textbf{Delete} removes all the students, courses and enrolments.  Notice that
the primary keys in every column family  are different in each run (create,
update, delete) in order to avoid  introducing biases to the results as product
of the tombstone delete paradigm  that Cassandra utilizes. That is, since
Cassandra does not completely  remove the primary keys of the inserted entities
(tombstone delete), reinsertion  using the same primary key might yield faster
times as the key already exists. After  each run, all column families (student,
course, and enrolment) are emptied and  ready for the next run.  The details  of
the \ac{CRUD} operations are explained further in the next sections.
		
\begin{description}
\item[Create] The create operation inserts all the students, courses and enrolments in
that precise order due to the nature of the referential integrity constraints
presented in Section~\ref{s:ed:ri}. The time required to insert all of the
entities in their respective column families  is recorded. In the Student and
Course column families, the insertion does not trigger any constraint validation
as these entities do not contain foreign keys. Contrarily, the insertion of
enrolments triggers foreign key validation checks on both \texttt{Student} and
\texttt{Course} column families.
\item[Update] The update operation is performed after the creation of all entities.
First, an attempt to update the primary key of each course is made. This
triggers constraint validations that result in exceptions thrown as the
\texttt{DeleteRule} of entity \texttt{Course} is \texttt{NODELETE}. Hence, the
times recorded for updating the Course column family represent the time required to identify a
constraint violation and throw the respective exceptions.
			
Next, the \texttt{Enrolment} column family is updated. In this case, the
courseId for each enrolment is changed to a different one ensuring that the distribution of
courses and students remains the same. The update on the enrolment column family
triggers foreign key validation checks to ensure that the course to which every
enrolment is being updated actually exists.
			
Finally, the primary key for each student is updated to a new integer value that
has never existed in the column family. This operation triggers a cascaded
update on the \texttt{Enrolment} column family by respectively updating the
student foreign key in its existing enrolments.
\item[Delete]The deletion of entities occurs first on the Enrolment column family,
where all of its records are deleted without requiring referential integrity
checks as this is a child entity. The times are recorded for such operation and
then all of the entities are reinserted with the same primary keys in order to
 assess the cascaded delete of students next.
		
Secondly, all of the students are deleted from their respective column family.
Given the cascaded delete rule of these entities, the validation handler ensures to delete first
all of the child entities before deleting an student. Hence, the times recorded
for this operation measure the time required for performing a cascaded delete on
the student depedencies in enrolment. Notice that the dependencies exist at this
point as they will have been reinserted in the previous step.
		
Finally, all of the courses are deleted. Despite the courses having a \texttt{NO
DELETE} rule, notice that at this point the enrolment column family is empty, so
courses can be deleted as there are no child dependencies. Thus, the times
recorded for this operation measure the constraint and referential integrity
checks as well as the delete operation of the respective entity. After this last
operation, all column families are emptied but all the primary keys still exist
due to tombstone. However, the whole keyspace is ready for the next batch of
operations as the primary keys of all column families will be different.


\end{description}		


\section{Analysis of results}
	Response time and throughput as function of response time.
	
--include how response itme and throughput became the measures. Commonly used
metrics.
--measuring throughput by logging it using our code. Can quote IBM on this. Most
benchmarks like TPC- cannot be used in our case as we dont use SQL and neither
is Cassandra an OLTP which supports transactions.

	Notice that external variables such as network latency, simultaneous processes
	in the operating systems of each node, and other variables are not considered
	for the analysis of results. Even when they are present, it is expected that
	results will not be biased by them. Nonetheless, the experiments will be
	performed at night time over a weekend as this is the time where the cluster is
	less used, thus reducing the presence of such variables and hence their impact
	by biasing the results \todo{or something like that :P}

\section{Summary}
	This chapter has presented the experimental design to evaluate the performance
	of each of the solutions and the api itself using as an example of application
	the toy problem used across this thesis. The experimental design involves
	assessing the performance of the CRUD operations on the different solutions
	proposed for referential integrity. The analysis of results is to be based on
	response time and throughput, two performance indicators that serve as
	guidelines for assessing the tradeoffs between the different solutions
	proposed.
	
	
	The next chapter presents the results and their discussions of the
	experimental design presented in this chapter
 






